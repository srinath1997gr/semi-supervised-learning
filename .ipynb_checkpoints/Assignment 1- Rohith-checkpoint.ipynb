{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-aling:center;color:Navy'>  Big Data Science - Fall 2023  </h1>\n",
    "<h1 style='text-aling:center;color:Navy'>  Assignment 1  </h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Submission Deadline: This assignment is due Friday, November 3 at 8:59 P.M.</b>\n",
    "\n",
    "A few notes before you start:\n",
    "- You are not allowed to use built-in libraries for co-training and label propagation itself.\n",
    "- Directly sharing answers is not okay, but discussing problems with other students is encouraged.\n",
    "- You should start early so that you have time to get help if you're stuck.\n",
    "\n",
    "- Complete all the exercises below and turn in a write-up in the form of a Jupyuter notebook, that is, an .ipynb file. The write-up should include your code and answers to exercise questions. You will submit your assignment online as an attachment (*.ipynb), through Canvas under Assignment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#3665af\">Semi-Supervised Learning </span>\n",
    "<hr>\n",
    "\n",
    "###### Goal\n",
    "In this assignment, we will explore the concepts and techniques of semi-supervised learning.\n",
    "\n",
    "###### Prerequisites\n",
    "This assignment has the following dependencies:\n",
    "- Jupyter Notebook, along with the following libraries (which should be installed on the Computing Platform):\n",
    "  - Scikit Learn\n",
    "  - Numpy\n",
    "  - os\n",
    "\n",
    "Let's dive into the world of semi-supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:30px;color:#3665af;background-color:#E9E9F5;padding:10px;\">Assignment Hands-on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;color:#F1F8FC;background-color:#0095EA;padding:10px;\"> Import libraries </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Dense,\n\u001b[0;32m      9\u001b[0m                                      Flatten,\n\u001b[0;32m     10\u001b[0m                                      Dropout,\n\u001b[0;32m     11\u001b[0m                                      BatchNormalization,\n\u001b[0;32m     12\u001b[0m                                      Conv2D,\n\u001b[0;32m     13\u001b[0m                                      MaxPooling2D,)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saved_model\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Sub-package for performing i/o directly instead of via ops in a graph.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\saved_model.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main_op\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m method_name_updater\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature_constants\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\main_op.py:22\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"SavedModel main op.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mBuilds a main op that defines the sequence of ops to be run as part of the\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mSavedModel load/restore operations.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain_op_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main_op\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain_op_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main_op_with_restore\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\main_op_impl.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\lookup_ops.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_lookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registration\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asset\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_ops\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_assert\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_assert\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense,\n",
    "                                     Flatten,\n",
    "                                     Dropout,\n",
    "                                     BatchNormalization,\n",
    "                                     Conv2D,\n",
    "                                     MaxPooling2D,)\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;color:#F1F8FC;background-color:#0095EA;padding:10px;\"> Learn more about the data </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are working with data collected from two different view from same region via satellite technology to study the Arctic region. These two data types offer valuable insights into various sea ice types, thereby enhancing navigation in the Arctic.\n",
    "\n",
    "1. **Sentinel-1 Data (view 1):** We use Synthetic Aperture Radar (SAR) satellite images from the Sentinel-1 mission. SAR images are incredibly useful for creating sea ice charts in the Arctic. SAR works by sending radar signals to the Earth's surface and capturing the signals that bounce back. One specific view we're utilizing is the Sentinel-1 image captured in HH polarization. This view helps us understand the characteristics of the sea ice in the Arctic. if you want to know more about it here is the [link](https://en.wikipedia.org/wiki/Sentinel-1).\n",
    "\n",
    "2. **AMSR2 Data (view 2):** Alongside each Sentinel-1 image, we have corresponding data from the Advanced Microwave Scanning Radiometer 2 (AMSR2). This dataset contains information about the brightness temperatures of the Earth's surface. AMSR2 measures microwave radiation, which can be used to gather information about surface properties like sea ice concentration. if you want to know more about it here is the [link](https://www.ospo.noaa.gov/Products/atmosphere/gpds/about_amsr2.html).\n",
    "\n",
    "To further analyze these datasets, we selected 10 files and divided the images into smaller patches, each patch 32 by 32 pixels. This patches allows us to focus on specific areas of interest within the Arctic and study them in detail. By combining the information from both Sentinel-1 and AMSR2 data, we can gain a comprehensive understanding of the Arctic environment and its sea ice patterns, which is crucial for various scientific and practical applications, including safe navigation in this challenging region.\n",
    "\n",
    "view 1: Sentinel-1 image\n",
    "\n",
    "<img alt=\"nersc_sar_primary view\" src=\"nersc_sar_primary.jpg\"/>\n",
    "\n",
    "view 2: AMSR2 image\n",
    "\n",
    "<img src=\"btemp_89_0h.jpg\" alt = \"btemp_89_0h view\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data.zip file from Canvas, and then execute the cell below to import the data. You can customize the directory name for the data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_directories(view1_dir, view2_dir, labels_dir):\n",
    "    \"\"\"\n",
    "    Load data from directories containing two views and corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "    - view1_dir (str): Path to the directory containing view 1 data files.\n",
    "    - view2_dir (str): Path to the directory containing view 2 data files.\n",
    "    - labels_dir (str): Path to the directory containing label data files.\n",
    "\n",
    "    Returns:\n",
    "    - view1_data (numpy.ndarray): NumPy array containing data from view 1.\n",
    "    - view2_data (numpy.ndarray): NumPy array containing data from view 2.\n",
    "    - labels_data (numpy.ndarray): NumPy array containing label data.\n",
    "\n",
    "    This function loads data from two views and their corresponding labels, assuming a common \"number\" part\n",
    "    in the file names for matching files. It ensures that data files from both views and labels are consistent\n",
    "    and loads them into NumPy arrays for further processing.\n",
    "    \"\"\"\n",
    "    view1_dir = 'C:/Users/srina/Documents/Big Data science/data/view1/' \n",
    "    view2_dir = 'C:/Users/srina/Documents/Big Data science/data/view2/' \n",
    "    labels_dir = 'C:/Users/srina/Documents/Big Data science/data/labels/' \n",
    "    # List all files in each directory\n",
    "    files_view1 = os.listdir(view1_dir)\n",
    "    files_view2 = os.listdir(view2_dir)\n",
    "    files_label = os.listdir(labels_dir)\n",
    "\n",
    "    # Initialize empty lists to store data from each view and labels\n",
    "    view1_data = []\n",
    "    view2_data = []\n",
    "    labels_data = []\n",
    "\n",
    "    # Iterate through the files in the directory\n",
    "    for filename in files_view1:\n",
    "        if filename.endswith('_samples_view1.npy'):\n",
    "            # Extract the common \"number\" part of the file name\n",
    "            common_number = filename.split('_')[0]\n",
    "\n",
    "            # Check if corresponding files exist for view2 and labels\n",
    "            if common_number + '_samples_view2.npy' in files_view2 and common_number + '_labels.npy' in files_label:\n",
    "                # Load data from the NumPy files\n",
    "                data_view1 = np.load(os.path.join(view1_dir, filename))\n",
    "                data_view2 = np.load(os.path.join(view2_dir, common_number + '_samples_view2.npy'))\n",
    "                data_labels = np.load(os.path.join(labels_dir, common_number + '_labels.npy'))\n",
    "\n",
    "                # Append data to respective lists\n",
    "                view1_data.append(data_view1)\n",
    "                view2_data.append(data_view2)\n",
    "                labels_data.append(data_labels)\n",
    "\n",
    "    view1_data = np.array(view1_data)\n",
    "    view2_data = np.array(view2_data)\n",
    "    labels_data = np.array(labels_data)\n",
    "\n",
    "    return view1_data, view2_data, labels_data\n",
    "\n",
    "\n",
    "view1_dir = 'view1' # change the directory as needed\n",
    "view2_dir = 'view2' # change the directory as needed\n",
    "labels_dir = 'labels' # change the directory as needed\n",
    "\n",
    "view1_data, view2_data, labels_data = load_data_from_directories(view1_dir, view2_dir, labels_dir)\n",
    "\n",
    "\n",
    "print(\" shape view 1 data: \", view1_data.shape)\n",
    "print(\" shape view 2 data: \", view2_data.shape)\n",
    "print(\" shape labels data: \", labels_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Part  1. Co-Training Models for Sea Ice Classification</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll be applying the Co-training technique to the dataset. Through this, you'll observe the outcomes of both semi-supervised learning and supervised learning when there's only a limited amount of labeled data available. You may want to revisit Lecture 02, which covers the topic of cotraining for a more comprehensive understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. Divide the dataset into three distinct sets: one for labeled data, one for unlabeled data, and one for test data. Make sure that the labeled dataset contains between 100 and 130 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code here. Comments are provided for guidance purposes. make adjustments as needed.\n",
    "def split_dataset(dataset_view1, dataset_view2, labels, labeled_size=120,test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into labeled, unlabeled, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (list or array-like): The input dataset to be split.\n",
    "    - labeled_size (int): The target size for the labeled set (default: 130).\n",
    "    - test_size (float): The proportion of the dataset to include in the test split (default: 0.2).\n",
    "    - random_seed (int): Seed for reproducibility (default: None).\n",
    "\n",
    "    Returns:\n",
    "    - labeled_set_view1: Subset of the dataset with labeled data (approximately 100-130 points).\n",
    "    - labeled_set_view2: Subset of the dataset with labeled data (approximately 100-130 points).\n",
    "    - label_labeled_set: Labels corresponding to the labeled data points.\n",
    "    - unlabeled_set: Subset of the dataset with unlabeled data.\n",
    "    - test_set: Subset of the dataset for testing.\n",
    "    - label_test_set: Labels corresponding to the test data points.\n",
    "    \"\"\"\n",
    "    # Split dataset into features and labels\n",
    "    X_view1 = dataset_view1\n",
    "    X_view2 = dataset_view2\n",
    "    y = labels\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(random_seed) \n",
    "    indices = np.random.permutation(len(X_view1))\n",
    "\n",
    "    X_view1 = X_view1[indices]\n",
    "    X_view2 = X_view2[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Split into labeled, unlabeled and test sets\n",
    "    labeled_view1 = X_view1[:labeled_size]\n",
    "    labeled_view2 = X_view2[:labeled_size]\n",
    "    label_labeled_set = y[:labeled_size]\n",
    "\n",
    "    unlabeled_view1 = X_view1[labeled_size:-int(len(X_view1)*test_size)]\n",
    "    unlabeled_view2 = X_view2[labeled_size:-int(len(X_view1)*test_size)]\n",
    "\n",
    "    test_view1 = X_view1[-int(len(X_view1)*test_size):]\n",
    "    test_view2 = X_view2[-int(len(X_view1)*test_size):]\n",
    "    label_test_set = y[-int(len(y)*test_size):]\n",
    "\n",
    "    return labeled_view1, labeled_view2, label_labeled_set, unlabeled_view1, unlabeled_view2, test_view1, test_view2, label_test_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labeled_view1, labeled_view2, labels, unlabeled_view1, unlabeled_view2, test_view1, test_view2, test_labels = split_dataset(view1_data, view2_data, labels_data, labeled_size=120, test_size=0.2, random_seed=42)\n",
    "print(labeled_view1.shape)\n",
    "print(labeled_view2.shape)\n",
    "print(labels.shape)\n",
    "print(unlabeled_view1.shape)\n",
    "print(unlabeled_view2.shape)\n",
    "print(test_view1.shape)\n",
    "print(test_view2.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2. initialize two classifiers for each view using scikit-learn. Consider using a Convolutional Neural Network (CNN) as one of the classifiers and a Random Forest as the other.\n",
    "Here's a short description of the configuration for the CNN (Convolutional Neural Network) and Random Forest (RF) classifiers to implement:\n",
    "\n",
    "**CNN Classifier Configuration:**\n",
    "\n",
    "1. Input Layer: BatchNormalization with input shape (32, 32, 1).\n",
    "2. Convolutional Layer 1: 32 filters, each with a 3x3 kernel and ReLU activation.\n",
    "3. Max Pooling Layer 1: 2x2 pooling with a stride of 2.\n",
    "4. Convolutional Layer 2: 32 filters, each with a 3x3 kernel and ReLU activation.\n",
    "5. Max Pooling Layer 2: 2x2 pooling with a stride of 2.\n",
    "6. Convolutional Layer 3: 32 filters, each with a 3x3 kernel and ReLU activation.\n",
    "7. Max Pooling Layer 3: 2x2 pooling with a stride of 2.\n",
    "8. BatchNormalization Layer.\n",
    "9. Flatten Layer.\n",
    "10. Dropout Layer with a dropout rate of 0.1.\n",
    "11. Fully Connected Layer 1: 16 neurons, ReLU activation, and L2 regularization with a weight decay of 0.001.\n",
    "12. Dropout Layer with a dropout rate of 0.1.\n",
    "13. Fully Connected Layer 2: 16 neurons, ReLU activation, and L2 regularization with a weight decay of 0.001.\n",
    "14. Dropout Layer with a dropout rate of 0.1.\n",
    "15. Output Layer: Dense layer with the number of neurons equal to the number of classes and softmax activation.\n",
    "\n",
    "**CNN Model Compilation:**\n",
    "- Optimizer: Adam\n",
    "- Loss Function: Sparse Categorical Crossentropy\n",
    "- Metrics for Evaluation: Accuracy\n",
    "\n",
    "**Random Forest Classifier Configuration:**\n",
    "- Number of Estimators: 20\n",
    "- Random State: 42\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "num_classes = 6\n",
    "# write your code here\n",
    "#implement classifiers based on the provided definition\n",
    "#cnn_classifier =\n",
    "#rf_classifier =\n",
    "# CNN Classifier\n",
    "cnn_classifier = keras.Sequential([\n",
    "\n",
    "  layers.BatchNormalization(input_shape=(32,32,1)),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(2, 2),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(2, 2),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  layers.BatchNormalization(),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  \n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(16, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(16, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.1),\n",
    "\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Random Forest Classifier \n",
    "rf_classifier = RandomForestClassifier(\n",
    "  n_estimators=20, \n",
    "  random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-3. Do the co-training part:\n",
    "   - Train classifiers on the labeled data\n",
    "   - Predict on the unlabeled data and identify instances that have a confidence score more than 90.\n",
    "   - Add the confident instances to the labeled set and train again\n",
    "   - Compute the accuracy of the classifiers on test set\n",
    "To provide a more understanding of the accuracy measure, please refer to the following link: [link](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
    "<img src=\"accuracy.jpg\" alt = \"accuracy metric\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1-3. Co-training\n",
    "import numpy as np\n",
    "\n",
    "# Flatten view2 data\n",
    "labeled_view2_flat = labeled_view2.reshape(labeled_view2.shape[0], -1)\n",
    "unlabeled_view2_flat = unlabeled_view2.reshape(unlabeled_view2.shape[0], -1)\n",
    "test_view2_flat = test_view2.reshape(test_view2.shape[0], -1)\n",
    "\n",
    "labeled_labels = np.squeeze(labels)\n",
    "\n",
    "def co_training(classifier1, classifier2, labeled_set1, labeled_set2, unlabeled_set1, unlabeled_set2, labels, test_set1, test_set2, test_labels, threshold_confidence=0.9):\n",
    "          \n",
    "    \"\"\"\n",
    "    Perform co-training with two classifiers on labeled and unlabeled data.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier1: The first classifier (e.g., CNN).\n",
    "    - classifier2: The second classifier (e.g., Random Forest).\n",
    "    - labeled_set (list or array-like): Labeled dataset.\n",
    "    - unlabeled_set (list or array-like): Unlabeled dataset.\n",
    "    - test_set (list or array-like): Test dataset.\n",
    "    - threshold_confidence (float): The minimum confidence threshold for adding unlabeled samples to the training set.\n",
    "\n",
    "    Returns:\n",
    "    - classifier1_accuracy (float): Accuracy of Classifier 1 on the test set after co-training.\n",
    "    - classifier2_accuracy (float): Accuracy of Classifier 2 on the test set after co-training.\n",
    "    \"\"\"\n",
    "    #for i in range(5):\n",
    "    classifier1.fit(labeled_set1, labels) \n",
    "    classifier2.fit(labeled_set2, labels)\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    pred_probs1 = classifier1.predict(unlabeled_set1)\n",
    "    pred_probs2 = classifier2.predict_proba(unlabeled_set2)\n",
    "\n",
    "    # Take max prob along classes axis\n",
    "    conf1 = pred_probs1.max(axis=1)  \n",
    "    conf2 = pred_probs2[:,1]\n",
    "\n",
    "    # Get predicted classes\n",
    "    pred_classes1 = classifier1.predict(unlabeled_set1)\n",
    "    pred_classes2 = classifier2.predict(unlabeled_set2)\n",
    "\n",
    "    # Flatten prediction arrays\n",
    "    pred_classes1 = pred_classes1.flatten()\n",
    "    pred_classes2 = pred_classes2.flatten()\n",
    "\n",
    "    confident_ind1 = np.where(conf1 > threshold_confidence)[0]\n",
    "    confident_ind2 = np.where(conf2 > threshold_confidence)[0]\n",
    "\n",
    "    print(confident_ind1.shape)\n",
    "    # Concatenate new confident samples to labeled sets\n",
    "    labeled_set1 = np.vstack((labeled_set1, unlabeled_set1[confident_ind1]))\n",
    "    labeled_set2 = np.vstack((labeled_set2, unlabeled_set2[confident_ind2]))\n",
    "    labels = np.squeeze(labels)\n",
    "    # Concatenate new confident labels\n",
    "    labels1 = np.concatenate((labels, pred_classes1[confident_ind1]))\n",
    "    labels2 = np.concatenate((labels, pred_classes2[confident_ind2]))\n",
    "\n",
    "    classifier1.fit(labeled_set1, labels1)\n",
    "    classifier2.fit(labeled_set2, labels2)\n",
    "\n",
    "    #classifier1_acc = classifier1_copy.evaluate(test_set1, test_labels)\n",
    "    predictions1 = classifier1.predict(test_set1)\n",
    "    predicted_classes1 = np.argmax(predictions1, axis=1)\n",
    "    classifier1_acc = accuracy_score(test_labels, predicted_classes1)\n",
    "    classifier2_acc = classifier2.score(test_set2, test_labels)\n",
    "\n",
    "    return classifier1_acc, classifier2_acc\n",
    "    \n",
    "cnn_acc, rf_acc = co_training(cnn_classifier, rf_classifier, labeled_view1, labeled_view2_flat, unlabeled_view1, unlabeled_view2_flat, \n",
    "                              labels, test_view1, test_view2_flat, test_labels)\n",
    "\n",
    "print(\"CNN Accuracy:\", cnn_acc)\n",
    "print(\"RF Accuracy:\", rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-4. pick one of the classifiers and do the supervised training with the labeled data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code here. Comments are provided for guidance purposes. make adjustments as needed.\n",
    "\n",
    "def supervised_training_and_accuracy(classifier, labeled_data, labeled_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Perform supervised training with a classifier on the labeled data and calculate the accuracy on test data.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier: The classifier to be used for supervised training (e.g., Random Forest).\n",
    "    - labeled_data (array-like): Labeled training data.\n",
    "    - labeled_labels (array-like): Labels for the labeled training data.\n",
    "    - test_data (array-like): Test data for evaluation.\n",
    "    - test_labels (array-like): Labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Accuracy of the classifier on the test data after supervised training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the classifier on the labeled data\n",
    "    classifier.fit(labeled_data, labeled_labels)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = classifier.predict(test_data)\n",
    "    \n",
    "    # Convert predictions to label classes\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    #test_labels = np.squeeze(test_labels)\n",
    "    # Calculate accuracy between predictions and true labels\n",
    "    accuracy = accuracy_score(test_labels, predicted_classes)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Usage\n",
    "# Choose one of the classifiers \n",
    "classifier = cnn_classifier \n",
    "\n",
    "# Get labeled data\n",
    "labeled_data = labeled_view1 \n",
    "\n",
    "# Get corresponding labels\n",
    "labeled_labels = labels\n",
    "\n",
    "# Get test data \n",
    "test_data = test_view1\n",
    "\n",
    "# Get test labels\n",
    "test_labels = test_labels\n",
    "\n",
    "# Calculate accuracy\n",
    "cnn_supervised_accuracy = supervised_training_and_accuracy(classifier, labeled_data, labeled_labels, test_data, test_labels)\n",
    "\n",
    "print(\"Accuracy:\", cnn_supervised_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-5. Compare the Co-training approach accuracy and supervised model with limited labeled data and write your reason about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Co-training CNN Accuracy:\", cnn_acc)\n",
    "print(\"Co-training RF Accuracy:\", rf_acc)\n",
    "print(\"Supervised CNN Accuracy:\", cnn_supervised_accuracy)\n",
    "\n",
    "# Compare accuracies\n",
    "if cnn_acc > cnn_supervised_accuracy:\n",
    "  print(\"Co-training CNN accuracy is higher than supervised CNN\")\n",
    "  \n",
    "if rf_acc > cnn_supervised_accuracy:\n",
    "  print(\"Co-training RF accuracy is higher than supervised CNN\")\n",
    "  \n",
    "# Reasons why co-training outperforms\n",
    "print(\"Reasons why co-training can outperform supervised learning:\")\n",
    "print(\"- Uses unlabeled data in addition to limited labeled data\") \n",
    "print(\"- Models teach each other through progressive self-labeling\")\n",
    "print(\"- Models have complementary inductive biases\")\n",
    "print(\"- Prevents overfitting to limited labeled data\") \n",
    "print(\"- Leverages multiple views of data for more robustness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Part 2. Label Propagation for Sea Ice Classification</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll be applying the label propagation technique to the dataset. Through this, you'll observe the outcomes of both semi-supervised learning and supervised learning when there's only a limited amount of labeled data available.\n",
    "\n",
    "\n",
    "<img src=\"label_propagation.jpg\" alt = \"label propgation process\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2-1. Apply the K-Nearest Neighbors (KNN) algorithm with a parameter configuration where n_neighbors is set to 7 for the label propagation model. Utilize one of the labeled data views and the corresponding unlabeled data from part 1 as input.\n",
    "To provide a more understanding of the accuracy measure, please refer to the following link: [link](https://en.wikipedia.org/wiki/Accuracy_and_precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code here. Comments are provided for guidance purposes. make adjustments as needed.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def label_propagation(labeled_data, unlabeled_data, labeled_labels, test_data, test_labels, n_neighbors=7):\n",
    "    \"\"\"\n",
    "    Apply K-Nearest Neighbors (KNN) to the label propagation model on one data view and test data.\n",
    "\n",
    "    Parameters:\n",
    "    - labeled_data (array-like): Labeled data points.\n",
    "    - unlabeled_data (array-like): Unlabeled data points.\n",
    "    - labeled_labels (array-like): Labels corresponding to the labeled data points.\n",
    "    - test_data (array-like): Test data to evaluate label propagation performance.\n",
    "    - label_test (array-like): Labels corresponding to the test data points.\n",
    "    - n_neighbors (int): Number of neighbors to consider in KNN (default: 7).\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Accuracy of label propagation on the test data.\n",
    "    \"\"\"\n",
    "    # Flatten labeled data \n",
    "    labeled_data = labeled_data.reshape(labeled_data.shape[0], -1)\n",
    "\n",
    "    # Flatten unlabeled data\n",
    "    unlabeled_data = unlabeled_data.reshape(unlabeled_data.shape[0], -1)\n",
    "    #Flatten Test Data\n",
    "    test_data = test_data.reshape(test_data.shape[0], -1)\n",
    "    labeled_labels = labeled_labels.ravel()\n",
    "\n",
    "    # Concatenate flattened labeled and unlabeled data\n",
    "    X = np.vstack((labeled_data, unlabeled_data)) \n",
    "    \n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Fit on combined data, with dummy 0 labels for unlabeled points\n",
    "    y = np.concatenate((labeled_labels, np.zeros(len(unlabeled_data))))\n",
    "    knn.fit(X, y) \n",
    "    \n",
    "    # Predict labels for unlabeled data\n",
    "    unlabeled_labels = knn.predict(unlabeled_data)\n",
    "    \n",
    "    # Refit KNN on combined labeled and propagated unlabeled labels\n",
    "    X = np.concatenate((labeled_data, unlabeled_data))\n",
    "    y = np.concatenate((labeled_labels, unlabeled_labels))\n",
    "    knn.fit(X, y) \n",
    "    \n",
    "    # Evaluate on test data\n",
    "    predictions = knn.predict(test_data)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "label_propagation_accuracy = label_propagation(labeled_view1, unlabeled_view1, labels, test_view1, test_labels)\n",
    "print(\"Label Propagation Accuracy:\", label_propagation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2. Select a classification algorithm and perform supervised learning on the labeled set. Then, evaluate the model's performance by calculating the accuracy. You can use a built-in library for the classifier. Compare your sepervised and semi supervised accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your code here. Comments are provided for guidance purposes. make adjustments as needed.\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def supervised_training_and_accuracy(classifier, labeled_data, labeled_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Perform supervised training with a classifier on the labeled data and calculate the accuracy on test data.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier: The classifier to be used for supervised training (e.g., Random Forest).\n",
    "    - labeled_data (array-like): Labeled training data.\n",
    "    - labeled_labels (array-like): Labels for the labeled training data.\n",
    "    - test_data (array-like): Test data for evaluation.\n",
    "    - test_labels (array-like): Labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Accuracy of the classifier on the test data after supervised training.\n",
    "    \"\"\"\n",
    "    # Train the ensemble classifier on the labeled data\n",
    "    classifier.fit(labeled_data, labeled_labels)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = classifier.predict(test_data)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "#usage\n",
    "# Define base classifiers\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "svm_classifier = SVC(probability=True, random_state=42)\n",
    "k_nearest_classifier = KNeighborsClassifier()\n",
    "    \n",
    "# Create a voting ensemble with the base classifiers\n",
    "ensemble_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', random_forest_classifier),\n",
    "    ('svm', svm_classifier),\n",
    "    ('knn', k_nearest_classifier)\n",
    "], voting='soft')  # 'soft' enables probability voting\n",
    "\n",
    "# Get labeled data\n",
    "labeled_data = labeled_view1.reshape(labeled_view1.shape[0], -1)\n",
    "\n",
    "# Get corresponding labels\n",
    "labeled_labels = labels\n",
    "\n",
    "# Get test data \n",
    "test_data = test_view1.reshape(test_data.shape[0], -1)\n",
    "\n",
    "# Get test labels\n",
    "test_labels = test_labels\n",
    "\n",
    "# Calculate accuracy\n",
    "ensemble_accuracy = supervised_training_and_accuracy(ensemble_classifier, labeled_data, labeled_labels, test_data, test_labels)\n",
    "\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px;color:#F1F8FC;background-color:#0095EA;padding:10px;\">Part 3. Now let's perform some experimentation and make some observations!</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-1. We will explore the impact of varying the threshold confidence in the co-training process at three different values: 80, 70, and 60. We will then assess the accuracy of co-training based on these threshold settings.\n",
    "To provide a more understanding of the accuracy measure, please refer to the following link: [link](https://en.wikipedia.org/wiki/Accuracy_and_precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Flatten view2 data\n",
    "labeled_view2_fl = labeled_view2.reshape(labeled_view2.shape[0], -1)\n",
    "unlabeled_view2_fl = unlabeled_view2.reshape(unlabeled_view2.shape[0], -1)\n",
    "test_view2_fl = test_view2.reshape(test_view2.shape[0], -1)\n",
    "# thresholds = [0.8, 0.7, 0.6]\n",
    "\n",
    "\"\"\"for threshold in thresholds:\n",
    "    cnn_acc, rf_acc = co_training(cnn_classifier, rf_classifier, labeled_view1, labeled_view2_fl, unlabeled_view1, unlabeled_view2_fl, \n",
    "                                  labels, test_view1, test_view2_fl, test_labels, threshold_confidence=threshold)\n",
    "\n",
    "    print(f\"Threshold Confidence {threshold}\")\n",
    "    print(\"CNN Accuracy:\", cnn_acc)\n",
    "    print(\"RF Accuracy:\", rf_acc)\n",
    "    print(\"\\n\")\"\"\"\n",
    "# Co-training thresholds to try\n",
    "thresholds = [0.8, 0.7, 0.6]\n",
    "\n",
    "# Initialize arrays to store accuracies\n",
    "cnn_accs = []\n",
    "rf_accs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "\n",
    "  # Co-training \n",
    "  cnn_acc, rf_acc = co_training(cnn_classifier, rf_classifier, \n",
    "                                labeled_view1, labeled_view2_fl, \n",
    "                                unlabeled_view1, unlabeled_view2_fl,  \n",
    "                                labels, test_view1, test_view2_fl, \n",
    "                                test_labels, threshold)\n",
    "  print(f\"Threshold Confidence {threshold}\")\n",
    "  print(\"CNN Accuracy:\", cnn_acc)\n",
    "  print(\"RF Accuracy:\", rf_acc)\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-2. Change the parameters of the K-Nearest Neighbors (KNN) algorithm for Label Propagation (part 2) with the values 3, 5, and 10, and explain what you understand about these parameter adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "n_neighbors = [3, 5, 10]\n",
    "\n",
    "for n_neighbors in n_neighbors:\n",
    "    accuracy = label_propagation(labeled_view1, unlabeled_view1, labels, test_view1, test_labels,n_neighbors )\n",
    "    print(f\"Accuracy with n_neighbors={n_neighbors}: {accuracy}\")\n",
    "\"\"\"Lower values like 3-5 have higher bias, less overfitting. Can misclassify outliers.\n",
    "Higher values like 10 have lower bias, more overfitting. Less affected by outliers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3-3. Let's see the impact of of a simplifies models for cotraining approach.\n",
    "- Reduce the number of convolutional layers in the question 1-2 from 3 to 1 convolution layer and the rest of the layers is the same\n",
    "- Change the number of trees for the random forest algorithm to 1.\n",
    "Evaluate the performance of cotraining approach.\n",
    "- Additionally, use the 1 layer convolution layer as the supervised model and evaluate the performance for supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Modified CNN Classifier with 1 convolution layer\n",
    "simple_cnn_classifier = keras.Sequential([\n",
    "\n",
    "  layers.BatchNormalization(input_shape=(32,32,1)),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  layers.BatchNormalization(),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "simple_cnn_classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "# Modified Random Forest Classifier with 1 tree\n",
    "rf_classifier_simple = RandomForestClassifier(\n",
    "    n_estimators=1, \n",
    "    random_state=42\n",
    ")\n",
    "# Co-training with simplified models\n",
    "cnn_acc_simple, rf_acc_simple = co_training(simple_cnn_classifier, rf_classifier_simple, labeled_view1, labeled_view2_flat, unlabeled_view1, unlabeled_view2_flat, \n",
    "                              labels, test_view1, test_view2_flat, test_labels)\n",
    "\n",
    "print(\"Co-training CNN Accuracy (Simplified):\", cnn_acc_simple)\n",
    "print(\"Co-training RF Accuracy (Simplified):\", rf_acc_simple)\n",
    "\n",
    "# Evaluate supervised learning with simplified CNN\n",
    "cnn_supervised_accuracy_simple = supervised_training_and_accuracy(simple_cnn_classifier, labeled_view1, labels, test_view1, test_labels)\n",
    "\n",
    "print(\"Supervised CNN Accuracy (Simplified):\", cnn_supervised_accuracy_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-4. Let's adjust the amount of labeled data in part 2 by considering two different quantities: 200 and 400 labeled data points. In each scenario, the remaining data will remain unlabeled. Evaluate the performance of label propagation under these labeled data scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "for labeled_size in [200, 400]:\n",
    "\n",
    "  # Label propagation\n",
    "  lp_acc = label_propagation(labeled_view1[:labeled_size], unlabeled_view1, \n",
    "                             labeled_labels[:labeled_size], test_view1, test_labels)\n",
    "                             \n",
    "  print(f\"Labeled Size: {labeled_size} - Accuracy: {lp_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3-5. Let's adjust the number of labeled data samples for part 1. Consider three scenarios: one with 200 labeled samples, another with 400 labeled samples, and a third with 600 labeled samples. In each scenario, the remaining data will remain unlabeled. Additionally, include an explanation of your understanding of how these parameter changes impact the algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "labeled_sizes = [200, 400, 600]\n",
    "\n",
    "for labeled_size in labeled_sizes:\n",
    "\n",
    "  # Split dataset\n",
    "  labeled_view1, labeled_view2, labels, unlabeled_view1, unlabeled_view2= split_dataset(view1_data, view2_data, labels_data, labeled_size=labeled_size)[:5]\n",
    "\n",
    "  # Flatening the data for Random Forest\n",
    "  labeled_view2_fla  = labeled_view2.reshape(labeled_view2.shape[0], -1)\n",
    "  unlabeled_view2_fla = unlabeled_view2.reshape(unlabeled_view2.shape[0], -1)\n",
    "  test_view2_fla = test_view2.reshape(test_view2.shape[0], -1)\n",
    "  # Co-training \n",
    "  cnn_acc, rf_acc = co_training(cnn_classifier, rf_classifier, \n",
    "                                labeled_view1, labeled_view2_fla,  \n",
    "                                unlabeled_view1, unlabeled_view2_fla,\n",
    "                                labels, \n",
    "                                test_view1, test_view2_fla, \n",
    "                                test_labels)\n",
    "                                \n",
    "  print(f\"Labeled Size: {labeled_size}\")                             \n",
    "  print(f\"CNN Accuracy: {cnn_acc}\")\n",
    "  print(f\"RF Accuracy: {rf_acc}\")\n",
    "# Labeled data sizes to try \n",
    "\"\"\"labeled_sizes = [200, 400, 600]\n",
    "\n",
    "for labeled_size in labeled_sizes:\n",
    "\n",
    "  # Split data into labeled and unlabeled\n",
    "  labeled_view1, labeled_view2, labels, unlabeled_view1, unlabeled_view2 = split_dataset(view1_data, view2_data, labels_data, \n",
    "                                                                                         labeled_size=labeled_size)\n",
    "\n",
    "  # Co-training \n",
    "  cnn_acc, rf_acc = co_training(...)\n",
    "\n",
    "  # Print accuracies\n",
    "  print(\"Labeled Size:\", labeled_size)\n",
    "  print(\"CNN Accuracy:\", cnn_acc)\n",
    "  print(\"RF Accuracy:\", rf_acc)\"\"\"\n",
    "\n",
    "# More labeled data improves accuracy by providing better initialization and supervision.  \n",
    "# But co-training can achieve reasonable accuracy with small labeled set by using unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3-6. Evalute the perfomance for different number of unlabeled data size.\n",
    "- Set labeled data size within the range of 100 to 130 and\n",
    "- Set the unlabeled data sizes at 200, 400, and 600.\n",
    "- Execute the algorithms and provide accuracy reports for both approaches: co-training and label propagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def split_dataset1(view1, view2, labels, \n",
    "                  labeled_size, unlabeled_size, test_size):\n",
    "                  \n",
    "  # Split labeled and test \n",
    "  labeled_view1 = view1[:labeled_size]\n",
    "  labeled_view2 = view2[:labeled_size]\n",
    "  labels = labels[:labeled_size]\n",
    "\n",
    "  test_view1 = view1[-int(len(view1)*test_size):] \n",
    "  test_view2 = view2[-int(len(view2)*test_size):]\n",
    "\n",
    "  # unlabeled is remaining samples\n",
    "  unlabeled_view1 = view1[labeled_size:-test_view1.shape[0]]\n",
    "  unlabeled_view2 = view2[labeled_size:-test_view2.shape[0]] \n",
    "  \n",
    "  # Truncate to unlabeled_size if given\n",
    "  if unlabeled_size:\n",
    "    unlabeled_view1 = unlabeled_view1[:unlabeled_size]\n",
    "    unlabeled_view2 = unlabeled_view2[:unlabeled_size]\n",
    "\n",
    "  return labeled_view1, labeled_view2, labels, unlabeled_view1, unlabeled_view2\n",
    "\n",
    "\n",
    "total_samples = len(view1_data)\n",
    "\n",
    "for unlabeled_size in [200, 400, 600]:\n",
    "\n",
    "  labeled_size = 120\n",
    "  \n",
    "  # Calculate unlabeled split size\n",
    "  #unlabeled_size = total_samples - labeled_size - int(total_samples*0.2)\n",
    "  \n",
    "  # Split dataset\n",
    "  labeled_view1, labeled_view2, labels, unlabeled_view1, unlabeled_view2 = split_dataset1(view1_data, view2_data, labels_data, labeled_size=labeled_size, unlabeled_size=unlabeled_size, test_size= 0.2) \n",
    "\n",
    "  labeled_view2_flaten = labeled_view2.reshape(labeled_view2.shape[0], -1)\n",
    "  unlabeled_view2_flaten = unlabeled_view2.reshape(unlabeled_view2.shape[0], -1)\n",
    "  test_view2_flaten = test_view2.reshape(test_view2.shape[0], -1)\n",
    "\n",
    "  # Co-training\n",
    "  cnn_acc, rf_acc = co_training(cnn_classifier, rf_classifier,\n",
    "                                labeled_view1, labeled_view2_flaten,  \n",
    "                                unlabeled_view1, unlabeled_view2_flaten,\n",
    "                                labels, \n",
    "                                test_view1, test_view2_flaten, \n",
    "                                test_labels)\n",
    "                                \n",
    "  print(f\"Unlabeled Size: {unlabeled_size}\")\n",
    "  print(f\"CNN Accuracy: {cnn_acc}\")\n",
    "  print(f\"RF Accuracy:{rf_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Submission</h2>\n",
    "\n",
    "<hr style=\"border-top: 5px solid orange; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "<p style=\"text-align: justify;\">You need to submit a Jupyter Notebook (*.ipynb) file that contains your completed code.\n",
    "\n",
    "\n",
    "<span>The file name should be in <strong>FirstName_LastName</strong> format</span>.</p>\n",
    "<p style=\"text-align: justify;\"><span>DO NOT INCLUDE EXTRA FILES, SUCH AS THE INPUT DATASETS</span>, in your submission;</p>\n",
    "<p style=\"text-align: justify;\">Please download your assignment after submission and make sure it is not corrupted or empty! We will not be responsible for corrupted submissions and will not take a resubmission after the deadline.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need Help?\n",
    "If you need help with this assignment, please get in touch with TAs via their emails, or go to their office hours.\n",
    "You are highly encouraged to ask your question on the designated channel for Assignment o on Microsoft Teams (not necessarily monitored by the instructor/TAs). Feel free to help other students with general questions. However, DO NOT share your solution.<hr style=\"border-top: 5px solid orange; margin-top: 1px; margin-bottom: 1px\"></hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
